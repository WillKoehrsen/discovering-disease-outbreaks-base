{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding places names in headlines\n",
    "First a few considerations: I'm not an experienced python coder, so the code you'll find here is far from what is considered \"good python\".\n",
    "\n",
    "Considerations aside, the code employs what could be thought as of a rather brute force approach to find places in the leadlines. Also, I can't say that it is performant, but I don't think the reference **liveProject** suggest this either.\n",
    "\n",
    "The way this works is:\n",
    "1. First it builds a list of words found in the headlines. It does some cleaning by removing a few words, as those were creating a lot of noise later on.\n",
    "2. The found words are concatenated in one giant regular expression.\n",
    "3. the geonamescache module is used to build a list of places. This places list is built in such a way that if a word extracted from the headlines is found in a particular place, the whole place name will be matched.\n",
    "4. Then, the list of found places is again searched in the headlines, in a per line basis. This assure us that we will find all places in all headlines.\n",
    "5. A final list of headelines, places and countries is build.\n",
    "6. A pandas dataframe is built based on the locations found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are parts of this Notebook that will be executed (Or not) based on this flag\n",
    "debug = False\n",
    "\n",
    "## Imports\n",
    "from unidecode import unidecode # To remove accents and stuff from texts\n",
    "import re                       #Regular expression library\n",
    "import geonamescache            # For database of places on earth\n",
    "from hashlib import md5         # Used to generate unique dictionary keys for the lines\n",
    "import numpy as np              # for array manipulation\n",
    "import pandas                   # Final resut needs to be in this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file with data\n",
    "data_file_path='../data/headlines.txt'\n",
    "#Read all lines in the headlines and \n",
    "## lines=[x.strip() for x in open(data_file_path).readlines()]\n",
    "## lines=\" \".join(lines)\n",
    "allLines=open(data_file_path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove accents and stuff from the text\n",
    "lines=unidecode(allLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a short list of the auto generated gregular expressions from words from the headlines:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[^#]*Exposure[^#]*|[^#]*Tests[^#]*|[^#]*Cholera[^#]*|[^#]*Chi[^#]*|[^#]*Rockville[^#]*'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find all words in the list that start with a caps letter, _or_ all words that are longer than 2\n",
    "wordsFound = re.findall(\"[A-Z][a-zA-Z]+|[a-zA-Z]{3,}\", lines)\n",
    "\n",
    "#This is a list of words that was hand generated by looking at a list of short words that just add noise\n",
    "removeTheseWords = ['san', 'hit', 'can', 'man']\n",
    "\n",
    "#Build list of probable city list, prepared for regex search\n",
    "wordsFound = [('[^#]*' + word.capitalize() + '[^#]*') for word in wordsFound if word.lower() not in removeTheseWords]\n",
    "wordsFound = list(set(wordsFound))\n",
    "print(\"Here's a short list of the auto generated gregular expressions from words from the headlines:\")\n",
    "'|'.join(wordsFound[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A word of consideration about geonamescache\n",
    "\n",
    "I think there are 2 considerations to have regarding this module.\n",
    "\n",
    "First off, there is no real reference from it on the itnernet (OR I wasn't able to find it).\n",
    "While looking for it a stumbled on a page that uses it in an example. Using that as a guide, I resorted to use `dir(geonamescache.GeonamesCache)` to find out by trial and error which methods were available.\n",
    "\n",
    "Second, his module is also (Or at least seems to be) incredible biased towards USA names, and based on the list of matches in the headlines it seems that that also applies to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries get 'normalized' by making names all small caps:\n",
      "\t ['andorra', 'united arab emirates', 'afghanistan', 'antigua and barbuda', 'anguilla']\n",
      "This is an example of how the text is generated from places so they can be extracted later:\n",
      "\n",
      "cities:  #Andorra la Vella# #Umm Al Quwain City# #Ras Al Khaimah City# #Zayed City# #Khawr Fakkan\n",
      "counties:  #Baldwin County# #Barbour County# #Bibb County# #Blount County# #Bullock County\n",
      "states:  #Alaska# #Alabama# #Arkansas# #Arizona# #California\n",
      "countries:  #Andorra# #United Arab Emirates# #Afghanistan# #Antigua and Barbuda# #Anguilla\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gc = geonamescache.GeonamesCache()\n",
    "#Get list of places from the GeonamesCache list\n",
    "\n",
    "# There was no real reference for the geonamescache library. I used this page below as main reference:\n",
    "#\n",
    "# https://galeascience.wordpress.com/2016/03/23/us-city-to-state-python-dictionary/\n",
    "#\n",
    "# Then I used dir(geonamescache.GeonamesCache) to find out about what moethods the library had available\n",
    "\n",
    "geodata = {}\n",
    "# Build a dictionary of places from geonamescache grouped by place type\n",
    "geodata['cities'] = [city['name'] for city in list(gc.get_cities().values())]\n",
    "geodata['counties'] = [county['name'] for county in list(gc.get_us_counties())]\n",
    "geodata['states'] = [state['name'] for state in list(gc.get_us_states().values())]\n",
    "geodata['countries'] = [country['name'] for country in list(gc.get_countries().values())]\n",
    "\n",
    "# This list will be used to detect country names in headlines\n",
    "countries = geodata['countries']\n",
    "allLocations = \"\"\n",
    "example = ''\n",
    "for geotype in geodata:\n",
    "    example = example + geotype + ': ' + unidecode(' #' + '# #'.join(geodata[geotype][:5])) + \"\\n\"\n",
    "    geodata[geotype] = unidecode(' #' + '# #'.join(geodata[geotype]))\n",
    "    allLocations = allLocations + geodata[geotype]\n",
    "\n",
    "countries = [country.lower() for country in countries]\n",
    "print(\"Countries get 'normalized' by making names all small caps:\\n\\t\", countries[:5])\n",
    "#countries[:5]\n",
    "print(\"This is an example of how the text is generated from places so they can be extracted later:\\n\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  8134 word matches in the list of places\n"
     ]
    }
   ],
   "source": [
    "# Find all worlds extracted from the headlines in the text of concatenated names\n",
    "# The way the system works, a word will match a full name:\n",
    "# E.g. if the word 'York' exists in the headlines word list, it will match all 'York', 'New York' and\n",
    "# 'New york City' from the list of places\n",
    "matches = re.findall('|'.join(wordsFound), allLocations)\n",
    "print('Found ', len(matches), 'word matches in the list of places')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no place in Norovirus Exposure in Hong Kong country: ( Hong Kong ) place: ( ['Hong Kong'] )\n",
      "Defaulting place name to country\n",
      "Found no place in Zika cases in Singapore reach 393 country: ( Singapore ) place: ( ['Singapore'] )\n",
      "Defaulting place name to country\n"
     ]
    }
   ],
   "source": [
    "lows = allLines.lower()\n",
    "found = []\n",
    "for place in matches:\n",
    "    placeLow = place.lower()\n",
    "    if placeLow in lows:\n",
    "        found.append((place, placeLow))\n",
    "allMyLines = allLines.split(\"\\n\")\n",
    "linesDict = {}\n",
    "# Dictionary of regular expressions\n",
    "regexpDict = {}\n",
    "for line in allMyLines:\n",
    "    if not line:\n",
    "        continue\n",
    "    # Generate an MD5 digest from the line.This is kind of a pet peeve; it also ensures there are\n",
    "    # no duplicated headlines\n",
    "    emmo = md5(line.lower().strip().encode('utf-8')).hexdigest()[:8]\n",
    "    #Brute force, come to my aid.\n",
    "    lineDict = {}\n",
    "    lineDict['line'] = line\n",
    "    lineDict['place'] = ''\n",
    "    lineDict['country'] = np.NaN\n",
    "    lineDict['places'] = []\n",
    "    lowLine = line.lower()\n",
    "    # Build list of headlines, places and countries.\n",
    "    for place, lowPlace in found:\n",
    "        if lowPlace not in regexpDict:\n",
    "            regexpDict[lowPlace] = re.compile(r\"\\b\" + lowPlace + r\"\\b\")\n",
    "        # Search for the place in the headline. If found, add it to the list of places found in the line\n",
    "        if regexpDict[lowPlace].search(lowLine):\n",
    "            # Place is a country?\n",
    "            if place.lower() in countries:\n",
    "                lineDict['country'] = place\n",
    "            else:\n",
    "                lineDict['places'].append(place)\n",
    "    # If we have no places in the headlines except the country, use the country as place\n",
    "    if lineDict['country'] is not np.NaN and len( lineDict['places'])<1:\n",
    "        lineDict['place'] = lineDict['country']\n",
    "        lineDict['places'] = [lineDict['country']]\n",
    "        # For debugging purposes: Only 2 occurrences\n",
    "        print(\"Found no place in\", line, \"country: (\", lineDict['country'], \") place: (\", lineDict['places'], \")\")\n",
    "        print(\"Defaulting place name to country\")\n",
    "    else:\n",
    "        # Remove duplicated words from list of places if they exist\n",
    "        lineDict['places'] = list(set(lineDict['places'])) # Did I say brute force? Brute force\n",
    "        # If we have only one place in the list use that\n",
    "        if len(lineDict['places']) == 1:\n",
    "            lineDict['place'] = lineDict['places'][0]\n",
    "        else:\n",
    "            # For lists longer than 1 element, give precedente to the longer name.\n",
    "            # So, 'New York City' will take precedence over both 'New York' and 'York', and\n",
    "            # 'New York' will take precedence over 'York'\n",
    "            placeLen = 0\n",
    "            realPlace = ''\n",
    "            for place in lineDict['places']:\n",
    "                currentPlaceLen = len(place)\n",
    "                if currentPlaceLen <= placeLen:\n",
    "                    continue\n",
    "                placeLen = currentPlaceLen\n",
    "                realPlace = place\n",
    "            lineDict['place'] = realPlace\n",
    "    if emmo not in linesDict: #We don't expect repeated lines, do we? Add headline to list\n",
    "        linesDict[emmo]=lineDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell was used to look at data and looking at patterns in order tro know which place to use\n",
    "if debug:\n",
    "    counter = 0\n",
    "    subcounter = 0\n",
    "    for place in linesDict:\n",
    "        lineData = linesDict[place]\n",
    "        foundItems = len(lineData['places'])\n",
    "        if foundItems < 1:\n",
    "            counter = counter + 1\n",
    "            print(counter, lineData['line'])\n",
    "        if foundItems > 1:\n",
    "            subcounter = subcounter - 1\n",
    "            print(subcounter, lineData['line'], lineData['places'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate list of short words to see which to strip out\n",
    "if debug:\n",
    "    allPlaces = []\n",
    "    allSmallPlaces = []\n",
    "    for place in linesDict:\n",
    "        #print(linesDict[place])\n",
    "        allPlaces = allPlaces + linesDict[place]['places']\n",
    "    allPlaces = set(allPlaces)\n",
    "    for place in allPlaces:\n",
    "        if len(place) < 5:\n",
    "            allSmallPlaces.append(place)\n",
    "    print(\"This is the list of short words I found:\\n\", \", \".join(allSmallPlaces))\n",
    "    #removeTheseWords = ['san', 'hit', 'can']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>cities</th>\n",
       "      <th>countries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Zika Outbreak Hits Miami</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Could Zika Reach New York City?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>First Case of Zika in Miami Beach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miami Beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Mystery Virus Spreads in Recife, Brazil</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>Recife</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 headline  cities      countries\n",
       "                 Zika Outbreak Hits Miami     NaN          Miami\n",
       "          Could Zika Reach New York City?     NaN  New York City\n",
       "        First Case of Zika in Miami Beach     NaN    Miami Beach\n",
       "  Mystery Virus Spreads in Recife, Brazil  Brazil         Recife"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert headlines to pandas dataframe\n",
    "headlinesArray = []\n",
    "for line in linesDict:\n",
    "    headline = linesDict[line]\n",
    "    headlinesArray.append([headline['line'], headline['country'], headline['place']])\n",
    "df = pandas.DataFrame(headlinesArray, columns = {'headline', 'countries', 'cities'})#.reset_index(drop=True)\n",
    "blankIndex=[''] * len(df)\n",
    "df.index=blankIndex\n",
    "df[:4]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
